{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c73cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b75e3ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be26ed9d-2e87-4f81-9998-be29454c798c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_built()) # Apple M-series metal-performance-shaders-framework\n",
    "print(torch.backends.mps.is_available()) # Apple M-series metal-performance-shaders-framework\n",
    "\n",
    "mps_device = default_device()\n",
    "print(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f0baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB) # https://docs.fast.ai/data.external.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61037931-6568-40f2-a3bd-b5919f7a1768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#5) [Path('/Users/kristianbotnen/.fastai/data/imdb/train/.DS_Store'),Path('/Users/kristianbotnen/.fastai/data/imdb/train/neg'),Path('/Users/kristianbotnen/.fastai/data/imdb/train/pos'),Path('/Users/kristianbotnen/.fastai/data/imdb/train/unsupBow.feat'),Path('/Users/kristianbotnen/.fastai/data/imdb/train/labeledBow.feat')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()\n",
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f04213-0434-4a7e-855a-6a50ba242272",
   "metadata": {},
   "source": [
    "<img src=\"imdb_finderview.png\" alt=\"IMDB dataset on disk\" width=\"200\"/>\n",
    "<img src=\"imdb_observationexample.png\" alt=\"IMDB dataset example\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56962c25-7c86-4389-9ede-d7cb12399178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "#from pathlib import Path\n",
    "\n",
    "def create_subset(src, dest, num_samples=256):\n",
    "    dest.mkdir(parents=True, exist_ok=True)\n",
    "    files = list(src.glob('*'))[:num_samples]\n",
    "    for file in files:\n",
    "        shutil.copy(file, dest/file.name)\n",
    "\n",
    "\n",
    "train_unsup = path/'unsup'\n",
    "train_pos = path/'train'/'pos'\n",
    "train_neg = path/'train'/'neg'\n",
    "test_pos = path/'test'/'pos'\n",
    "test_neg = path/'test'/'neg'\n",
    "\n",
    "# Create subset directories\n",
    "top_datapath = path.parent\n",
    "subset_path = top_datapath/'subset'\n",
    "\n",
    "(subset_path/'unsup').mkdir(parents=True, exist_ok=True)\n",
    "(subset_path/'train'/'pos').mkdir(parents=True, exist_ok=True)\n",
    "(subset_path/'train'/'neg').mkdir(parents=True, exist_ok=True)\n",
    "(subset_path/'test'/'pos').mkdir(parents=True, exist_ok=True)\n",
    "(subset_path/'test'/'neg').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy files to subset directories\n",
    "create_subset(train_unsup, subset_path/'unsup')\n",
    "create_subset(train_pos, subset_path/'train'/'pos')\n",
    "create_subset(train_neg, subset_path/'train'/'neg')\n",
    "create_subset(test_pos, subset_path/'test'/'pos')\n",
    "create_subset(test_neg, subset_path/'test'/'neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d129472a-fc7e-4188-adbe-677a8f8fd677",
   "metadata": {},
   "source": [
    "<img src=\"imdb_subset_finderview.png\" alt=\"IMDB dataset on disk\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ae801cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset. Both the training set and the validation set.\n",
    "datablock = DataBlock(\n",
    "    blocks=(TextBlock.from_folder(subset_path), CategoryBlock), # Input is text, Output is categories (positive / negative).\n",
    "    get_items=get_text_files, # Get text files in path recursively, only in folders, if specified.\n",
    "    splitter=GrandparentSplitter(valid_name='test'), # Split items from the grand parent folder names (train_name and valid_name).\n",
    "    get_y=parent_label, # Label item with the parent folder name.\n",
    ")\n",
    "\n",
    "dataloaders = datablock.dataloaders(subset_path, bs=16, device=mps_device) # https://docs.fast.ai/data.transforms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2fe3c0e-4efe-4cc3-b479-229ed90ebab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting-up type transforms pipelines\n",
      "Collecting items from /Users/kristianbotnen/.fastai/data/subset\n",
      "Found 1536 items\n",
      "2 datasets of sizes 512,512\n",
      "Setting up Pipeline: Tokenizer -> Numericalize\n",
      "Setting up Pipeline: parent_label -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n",
      "\n",
      "Building one sample\n",
      "  Pipeline: Tokenizer -> Numericalize\n",
      "    starting from\n",
      "      /Users/kristianbotnen/.fastai/data/subset/train/neg/1821_4.txt\n",
      "    applying Tokenizer gives\n",
      "      ['xxbos', 'xxmaj', 'working', 'with', 'one', 'of', 'the', 'best', 'xxmaj', 'shakespeare', 'sources', ',', 'this', 'film', 'manages', 'to', 'be', 'creditable', 'to', 'it', \"'s\", 'source', ',', 'whilst', 'still', 'appealing', 'to', 'a', 'wider', 'audience', '.', '\\n\\n', 'xxmaj', 'branagh', 'steals', 'the', 'film', 'from', 'under', 'xxmaj', 'fishburne', \"'s\", 'nose', ',', 'and', 'there', \"'s\", 'a', 'talented', 'cast', 'on', 'good', 'form', '.']\n",
      "    applying Numericalize gives\n",
      "      TensorText of size 54\n",
      "  Pipeline: parent_label -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n",
      "    starting from\n",
      "      /Users/kristianbotnen/.fastai/data/subset/train/neg/1821_4.txt\n",
      "    applying parent_label gives\n",
      "      neg\n",
      "    applying Categorize -- {'vocab': None, 'sort': True, 'add_na': False} gives\n",
      "      TensorCategory(0)\n",
      "\n",
      "Final sample: (TensorText([   2,    8,  739,   30,   44,   14,    9,  138,    8, 3284, 5455,\n",
      "              11,   20,   32,  866,   15,   43,    0,   15,   18,   23, 3134,\n",
      "              11, 1661,  150, 3073,   15,   13,    0,  313,   10,   25,    8,\n",
      "               0, 1483,    9,   32,   53,  454,    8,    0,   23, 3883,   11,\n",
      "              12,   56,   23,   13, 1352,  184,   36,   68,  711,   10]), TensorCategory(0))\n",
      "\n",
      "\n",
      "Collecting items from /Users/kristianbotnen/.fastai/data/subset\n",
      "Found 1536 items\n",
      "2 datasets of sizes 512,512\n",
      "Setting up Pipeline: Tokenizer -> Numericalize\n",
      "Setting up Pipeline: parent_label -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n",
      "Setting up after_item: Pipeline: ToTensor\n",
      "Setting up before_batch: Pipeline: Pad_Chunk -- {'pad_idx': 1, 'pad_first': True, 'seq_len': 72}\n",
      "Setting up after_batch: Pipeline: \n",
      "\n",
      "Building one batch\n",
      "Applying item_tfms to the first sample:\n",
      "  Pipeline: ToTensor\n",
      "    starting from\n",
      "      (TensorText of size 54, TensorCategory(0))\n",
      "    applying ToTensor gives\n",
      "      (TensorText of size 54, TensorCategory(0))\n",
      "\n",
      "Adding the next 3 samples\n",
      "\n",
      "Applying before_batch to the list of samples\n",
      "  Pipeline: Pad_Chunk -- {'pad_idx': 1, 'pad_first': True, 'seq_len': 72}\n",
      "    starting from\n",
      "      [(TensorText of size 54, TensorCategory(0)), (TensorText of size 234, TensorCategory(0)), (TensorText of size 165, TensorCategory(0)), (TensorText of size 494, TensorCategory(0))]\n",
      "    applying Pad_Chunk -- {'pad_idx': 1, 'pad_first': True, 'seq_len': 72} gives\n",
      "      ((TensorText of size 494, TensorCategory(0)), (TensorText of size 494, TensorCategory(0)), (TensorText of size 494, TensorCategory(0)), (TensorText of size 494, TensorCategory(0)))\n",
      "\n",
      "Collating items in a batch\n",
      "\n",
      "No batch_tfms to apply\n"
     ]
    }
   ],
   "source": [
    "datablock.summary(subset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19880255-f876-4326-b9a9-4a5278a3d83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj that word ' true ' in this film 's title got my alarm xxunk xxunk . xxmaj they rang xxunk when a title card xxunk to xxmaj america 's xxmaj civil xxmaj war as the ' war xxmaj between the xxmaj states ' ( the xxunk preferred by die - hard xxunk ) . xxmaj jesse xxmaj james -- thief , slave - xxunk and murderer -- is described as a quiet , gentle farm boy . \\n\\n xxmaj how dishonest is this movie ? xxmaj there is xxup no mention of slavery , far less of the documented fact that xxmaj jesse xxmaj james 's poor xxunk mother owned xxunk before the war , and that xxmaj jesse and his brother xxmaj frank actively fought to xxunk slavery . xxmaj according to this movie , all those xxmaj civil xxmaj war soldiers were really fighting to decide</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxmaj this 1996 movie was the first adaptation of xxmaj jane xxmaj eyre that i ever watched and when i did so i was xxunk by it . xxmaj so much of the novel had been left out and i considered xxmaj william xxmaj hurt to be terribly miscast as xxmaj rochester . xxmaj since then i have watched all the other noteworthy adaptations of the novel , the three short versions of ' xxunk , ' 70 and ' 97 and the three mini series of ' xxunk , ' xxunk and 2006 , and i have noticed that there are worse adaptations and worse xxmaj xxunk . \\n\\n xxmaj this is without doubt the most exquisite xxmaj jane xxmaj eyre adaptation as far as cinematography is concerned . xxmaj director xxmaj franco xxmaj xxunk xxunk in beautiful long shots of snow falling from a winter sky ,</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxmaj at the xxunk of the ' celebrity xxmaj big xxmaj brother ' racism row in 2007 ( involving xxmaj xxunk xxmaj xxunk and the late xxmaj xxunk xxmaj goody ) , i condemned on an internet forum those ' xxunk . ' fans who xxunk the show , after years of xxunk ' racist ' ' 70 's sitcoms such as ' xxunk &amp; xxmaj chips ' &amp; ' love xxmaj xxunk xxmaj xxunk ' . i thought they were being hypocritical , and said so . ' it xxmaj ai n't xxmaj half xxmaj hot xxmaj mum ' was then thrown into the argument , with some pointing out it had starred an xxmaj english actor xxunk - up . xxmaj well , yes , but xxmaj michael xxmaj bates had lived in xxmaj india as a boy , and spoke xxmaj xxunk xxunk . xxmaj the</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloaders.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "133b2a06-bf43-4aba-acdf-9fe28332c303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastai.data.core.DataLoaders'>\n",
      "2\n",
      "512 512\n",
      "(TensorText([   2,    8,  739,   30,   44,   14,    9,  138,    8, 3284, 5455,\n",
      "              11,   20,   32,  866,   15,   43,    0,   15,   18,   23, 3134,\n",
      "              11, 1661,  150, 3073,   15,   13,    0,  313,   10,   25,    8,\n",
      "               0, 1483,    9,   32,   53,  454,    8,    0,   23, 3883,   11,\n",
      "              12,   56,   23,   13, 1352,  184,   36,   68,  711,   10]), TensorCategory(0))\n",
      "(TensorText([   2,    8,   88,   71, 5475,    7,   19,   11,    9,  218,  677,\n",
      "             152,   17, 2834,   12,   19,  305,    9,   27,  199,  653,   15,\n",
      "             126,   10,  206,   11,   47, 6665,   15,  117, 5475,    7, 1241,\n",
      "              12,    5,  156,   19,   10,    8, 1795,   87,   11,  179,  115,\n",
      "             677,  173, 2941,  208,  125,   47, 2083,    9,   98,   44,   11,\n",
      "              19,  490,   11, 2096,    0,    5,  156,   72,    8,  172,   11,\n",
      "              91,    8,  559,  330,   16, 3120,   14, 6666,    9, 1067,   22,\n",
      "             155,   17,    8,    0,  368,   70,   47,  797,  178,   44,   14,\n",
      "             164,    0,    0,   14,   13,   27,   72,   22,    8, 5475,    7,\n",
      "            4647,   78,   37,   43,  985,   13,  111,   27,   11,   17,  195,\n",
      "              18,   78,   37,   43,   74,  985,   54,    0,   14,   13,  111,\n",
      "              27,   11,   29,   18,  500,   73,   15,   75,   99,   21,   10,\n",
      "               8,   28,   19, 2675,   15,  786,  170,   18,   11,   19, 1988,\n",
      "              21,   79,  570,  677,   15,    0,   11,   12,   19, 5476,    0,\n",
      "              21,    9,  128,  287,   53,    9, 1199,   70, 6667,  183,    9,\n",
      "               7,  264,   12,  523,   87,   10,   86,   47,   80,  120,   21,\n",
      "               0,    9,   96,   52,   90,    0,   17,    9,   97,  115,   73,\n",
      "              28,  170,   47,   93,  186,   53,    9, 1443,  973,   70,  117,\n",
      "               9,   27,   38,   30,    9, 1727,  181,    0,   34,  122,  151,\n",
      "              72, 1254,   11,   19,   70,  130, 1937, 2146,   20,   27,   11,\n",
      "              19,  490,   11,   56,   40, 6668,  748,   15,  185,  216,   21,\n",
      "            2874,   88,   10]), TensorCategory(0))\n",
      "(TensorText([   2,    8, 6685,   50,    8,   20,   44,   26,   13,  242, 1369,\n",
      "              15,  786,  183,   10,    8,   18,   63,   13, 1036,   12, 1075,\n",
      "             954,   11,   31,   18,   46,  292,   15,  495,   53,   56,   10,\n",
      "               8, 3526,    8,    0,   16,  241,  222, 5188,   12,  579,   11,\n",
      "              12,   39,   94,   37, 3995,   17,   20,   44,   10,    8, 4655,\n",
      "               8,    0,    8,    0,   12,    8,  328,    8, 6686,  750,   17,\n",
      "            1071,  509,  416,   10,    8, 1536,    8, 2976,   12,    8, 2256,\n",
      "               8, 6537,   11,  756,  441,   11,  247,   18,  113,  140,    9,\n",
      "             415,   10,   19,  133,   21,   23,    9,  113,    9,  601,   82,\n",
      "             458,   11,   12,   18,   23,  253,   15, 1777,  175,   11,   69,\n",
      "               9,  243,   12,  166,   41,  118,   58,  157, 6687,   10,    8,\n",
      "              64,   33,  186,  173,   15,   41, 3537,  121,   11,   21,   23,\n",
      "               7,  576,   11,   31,   33,  237, 3113,   41,   68,  825,   15,\n",
      "             117,   18,  203,   10,    8,   18,   86,   35,  154,   10,    8,\n",
      "             603,  269, 1701,   53,   20,   27,   64,   45,   46,  707,   10]), TensorCategory(0))\n"
     ]
    }
   ],
   "source": [
    "print(type(dataloaders))\n",
    "print(len(dataloaders))\n",
    "print(len(dataloaders.train_ds), len(dataloaders.valid_ds))\n",
    "\n",
    "for i, sample in enumerate(dataloaders.train_ds):\n",
    "    print(sample)\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea4e74-b02c-4c42-8ba7-67a001e961c1",
   "metadata": {},
   "source": [
    "## Train and tune our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b9ff5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pythonki/lib/python3.12/site-packages/fastai/text/learner.py:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  wgts = torch.load(wgts_fname, map_location = lambda storage,loc: storage)\n"
     ]
    }
   ],
   "source": [
    "# Train and tune our model.\n",
    "learn = text_classifier_learner(dataloaders, AWD_LSTM, drop_mult=0.5, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4382d8b4-29b4-4065-a68f-9b54dcc2a15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.604538</td>\n",
       "      <td>0.605831</td>\n",
       "      <td>0.673828</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.470686</td>\n",
       "      <td>0.502423</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>00:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.380160</td>\n",
       "      <td>0.396072</td>\n",
       "      <td>0.832031</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.293321</td>\n",
       "      <td>0.376568</td>\n",
       "      <td>0.833984</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.205844</td>\n",
       "      <td>0.353899</td>\n",
       "      <td>0.841797</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fine_tune(4, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f25f0bad-33b6-4de2-ae9f-c94982666e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos i really wanted to be able to give this film a 10 . xxmaj i 've long thought it was my favorite of the four modern live - action xxmaj batman films to date ( and maybe it still will be -- i have yet to watch the xxmaj schumacher films again ) . xxmaj i 'm also starting to become concerned about whether xxmaj i 'm somehow xxunk being xxunk . xxmaj you see , i always liked the xxmaj schumacher films . xxmaj as far as i can remember , they were either xxunk or xxunk to me . xxmaj but the conventional wisdom is that the two xxmaj tim xxmaj burton directed films are far superior . i had serious problems with the first xxmaj burton xxmaj batman this time around -- i ended up giving it a 7 - xxunk xxunk as i might ,</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos i wrote this as a two part review . xxmaj part two has spoilers . \\n\\n xxmaj part 1 : \\n\\n xxmaj no , this is n't that one about the sex with car xxunk . xxmaj this is the one about racism in xxup l.a . xxmaj you know , the one where everybody is a racist , and race is the topic on everybody 's mind at all times . xxmaj race . \\n\\n xxmaj its like the movie has a form of xxunk xxunk where race is the constant theme . xxmaj race . xxmaj racist . xxmaj racism . xxmaj race xxmaj relations . xxmaj xxunk race . \\n\\n xxmaj paul xxmaj xxunk made a movie which took the structure of xxmaj magnolia , which was used to show the xxunk of people who are xxunk connected , and then screwed it into a xxunk</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos a film that tends to get buried under xxunk and xxunk - xxmaj it 's a remake ! xxmaj doris xxmaj day is in it ! xxmaj she sings ! - xxmaj hitchcock 's second crack at ' the xxmaj man xxmaj who xxmaj knew xxmaj too xxmaj much ' is his most under - rated film , and arguably a fully xxunk masterpiece in its own right . \\n\\n xxmaj this is , in more ways than one , xxmaj doris xxmaj day 's film . xxmaj not only does she give the finest performance of her career , more than holding her own against xxmaj james xxmaj stewart , but the whole film is subtly structured around her character rather than his . xxmaj this is , after all , a film in which music is both xxunk and plot device . xxmaj what better casting than</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos xxmaj wrestlemania 2 is the only xxmaj xxunk xxunk to be held at three different locations , and xxmaj while it was an interesting idea , it did n't really work . xxmaj there are only really two matches that really struck out , with the rest being decent , or most of them , pretty terrible . xxmaj there are some entertaining celebrity 's on hand , like xxmaj susan xxmaj saint xxmaj james , xxmaj ray xxmaj charles and xxmaj xxunk xxmaj xxunk , but the experience was a waste of time for the most part . xxmaj the xxmaj british xxmaj xxunk xxmaj vs xxmaj the xxmaj dream xxmaj team match , is worth the price of admission itself , but you can honestly see that anywhere . \\n\\n xxmaj matches . \\n\\n xxmaj xxunk xxmaj xxunk . \\n\\n xxmaj paul xxmaj xxunk xxmaj vs xxmaj</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos xxmaj xxunk xxmaj xxunk has become famous to the world after his marvelous production xxup the xxup xxunk . xxmaj movie fans got to know the style of the director who introduced himself as one among the post war new xxunk , an aristocrat who developed his individual free thinking and , xxunk , expressed them as an artist . xxmaj however , when applied to this movie , xxup morte a xxup venezia based upon the novel by xxmaj thomas xxmaj mann , it 's a slightly different story . \\n\\n xxmaj the entire film is , at first view , so unique , so psychological and so much influenced by the various thoughts of an artist ( both director and main character xxmaj gustav von xxmaj xxunk ) that it seems to be \" unwatchable \" for many viewers . xxmaj therefore , such opinions about the</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos xxmaj talk xxmaj radio sees a man somewhat accidentally stumble through life , indeed the xxmaj american xxmaj dream , from whatever xxunk - standard and everyday job he has in a store ; to xxunk of a local radio show before going right the way through to the same job only later xxunk nationwide . xxmaj it 's a role he adopts out of his own xxunk and natural mannerisms , a xxunk mad approach to freedom of speech as he attacks just about everyone and everything , even those that often call up to agree with him or compliment him . xxmaj his role as a man that xxunk on all things good , evil , right , wrong , political , religious , moral and immoral is something that people seem to take to in one form ; that of ' it 's entertaining and worth</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxbos \" man of the xxmaj year \" tells the story of xxmaj tom xxmaj dobbs ( robin xxmaj williams ) a political comedian ( like xxmaj jon xxmaj stewart or xxmaj stephen xxmaj xxunk ) who has his own television show . xxmaj on his show he talks about all sorts of things but his main focus are political issues which he is very xxunk about . xxmaj one day on his show , a fan from the audience raises the idea that xxmaj dobbs should run for xxmaj president of the xxmaj united xxmaj states . xxmaj after that episode aired , millions xxunk to the web to create various xxunk and voice their opinions on why xxmaj dobbs would make a great candidate for the xxmaj president for the xxmaj united xxmaj states . a few weeks later , xxmaj dobbs decides to run for xxmaj president</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos xxmaj yeah , a long time ago it turned into a tourist attraction . xxmaj now it 's a prison again . xxmaj kind of . xxmaj well , it 's more like an xxunk mixed together with a junior high school but there are lots of guys running around wearing orange xxunk , so i guess in that way it 's like a prison . xxmaj not really though . xxmaj when xxmaj xxunk , xxmaj steven xxmaj seagal 's character , is being admitted into prison , he 's standing xxunk in line and wanders over to a different line so he can talk to his friend , like he 's in line for the security check at the xxunk . xxmaj then before too long he and his friend are throwing punches , xxunk around a couple of security xxunk . \\n\\n xxmaj let me tell</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xxbos xxmaj this is an excellent example of an xxunk bad b - movie . xxmaj there are worse movies than this one ( titanic for example ) , but this definitely shares the pile of steaming crap movies . \\n\\n xxup ok this was apparently shot in xxmaj kansas xxmaj city , which explains why everyone is so lame . xxmaj the main guy looks like xxmaj steve xxmaj guttenberg , and is even more lame than him ! i did n't even think that was possible ! xxmaj in fact , him and the main girl in the movie are responsible for the xxup worst xxup drama xxup ever ! xxmaj its not just that there acting was w xxrep 9 a y over - dramatic , well actually it was , of course the script was terrible which combines for a deadly one - two punch in</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2b7aebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a: pos.\n",
      "Probability it's a positive: 0.8641\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a: neg.\n",
      "Probability it's a positive: 0.3807\n"
     ]
    }
   ],
   "source": [
    "# Use our model by passing it a review.\n",
    "category,_,probs = learn.predict(\"I really liked that movie\")\n",
    "\n",
    "print(f\"This is a: {category}.\")\n",
    "print(f\"Probability it's a positive: {probs[1]:.4f}\")\n",
    "\n",
    "category,_,probs = learn.predict(\"I did not like that movie, it was awful. It was the worst thing I have ever seen\")\n",
    "\n",
    "print(f\"This is a: {category}.\")\n",
    "print(f\"Probability it's a positive: {probs[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c852fc02-7806-4fcd-a631-c0f6a82fdc84",
   "metadata": {},
   "source": [
    "<img src=\"cpu_gpu_belastning.png\" alt=\"CPU og GPU belastning\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015a02d-f657-4e6e-b005-ee0113d5a724",
   "metadata": {},
   "source": [
    "## ULMFiT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046e8d44-36ed-4d8b-a275-7a6ef1c4f212",
   "metadata": {},
   "source": [
    "<img src=\"ulmfit.png\" alt=\"ULMFiT process\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a94f690-358d-4f07-9dca-a5f69a544b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_lm = TextDataLoaders.from_folder(subset_path/'unsup', is_lm=True, valid_pct=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "140d0536-5b62-4a99-bff7-00965b687a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj this xxunk is the worst show i have ever seen on xxup tv . xxmaj ever . xxmaj and i watch a lot of xxup tv . xxmaj it basically deals with a bunch of trashy , low class and xxunk xxunk women xxunk for the \" love \" of xxunk up xxunk xxmaj xxunk xxmaj xxunk . i hope most people watch it for its xxunk or even xxunk</td>\n",
       "      <td>xxmaj this xxunk is the worst show i have ever seen on xxup tv . xxmaj ever . xxmaj and i watch a lot of xxup tv . xxmaj it basically deals with a bunch of trashy , low class and xxunk xxunk women xxunk for the \" love \" of xxunk up xxunk xxmaj xxunk xxmaj xxunk . i hope most people watch it for its xxunk or even xxunk xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxunk without easily being xxunk . \\n\\n i wonder how many more times i will watch \" the xxmaj age of xxmaj xxunk \" before i xxunk being exposed to xxmaj hollywood 's xxunk century xxunk , such as \" xxunk xxmaj day \" or \" wild , xxmaj wild xxmaj west \" . xxmaj all i know is that xxmaj ellen xxmaj olenska ( as one of my favorite cinematic xxunk</td>\n",
       "      <td>without easily being xxunk . \\n\\n i wonder how many more times i will watch \" the xxmaj age of xxmaj xxunk \" before i xxunk being exposed to xxmaj hollywood 's xxunk century xxunk , such as \" xxunk xxmaj day \" or \" wild , xxmaj wild xxmaj west \" . xxmaj all i know is that xxmaj ellen xxmaj olenska ( as one of my favorite cinematic xxunk )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>films . \\n\\n xxmaj my favourite director is xxmaj john xxmaj xxunk xxmaj my favourite actor is xxmaj chow yun - fat . i like films with xxunk xxunk xxunk . i like long action sequences . i like a slight bit of martial arts but not too much . xxmaj good photography . xxmaj the sense that the director gave a damn about the film . xxmaj the sense that the</td>\n",
       "      <td>. \\n\\n xxmaj my favourite director is xxmaj john xxmaj xxunk xxmaj my favourite actor is xxmaj chow yun - fat . i like films with xxunk xxunk xxunk . i like long action sequences . i like a slight bit of martial arts but not too much . xxmaj good photography . xxmaj the sense that the director gave a damn about the film . xxmaj the sense that the actors</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloaders_lm.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cbde531-142c-4ce0-919d-09ecb60c89c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pythonki/lib/python3.12/site-packages/fastai/text/learner.py:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  wgts = torch.load(wgts_fname, map_location = lambda storage,loc: storage)\n"
     ]
    }
   ],
   "source": [
    "llm_learn = language_model_learner(dataloaders_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], path=subset_path/'unsup', wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "617f73d6-d6c1-412f-9d69-3963847fb4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.261617</td>\n",
       "      <td>3.936362</td>\n",
       "      <td>0.268338</td>\n",
       "      <td>51.231895</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.099275</td>\n",
       "      <td>3.833188</td>\n",
       "      <td>0.271326</td>\n",
       "      <td>46.209633</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.956356</td>\n",
       "      <td>3.793528</td>\n",
       "      <td>0.272511</td>\n",
       "      <td>44.412815</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.847576</td>\n",
       "      <td>3.783887</td>\n",
       "      <td>0.271635</td>\n",
       "      <td>43.986694</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_learn.fit_one_cycle(4, 1e-2) # 0.01 | https://iconof.com/1cycle-learning-rate-policy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e17beae-e169-4a4f-8d8f-2770dbaaff19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/Users/kristianbotnen/.fastai/data/subset/unsup/models/4epoch.pth')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_learn.save('4epoch')\n",
    "# llm_learn = llm_learn.load('1epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "428aeaa7-fd49-4dbc-89cf-0b28297a410f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.659177</td>\n",
       "      <td>3.765779</td>\n",
       "      <td>0.275541</td>\n",
       "      <td>43.197353</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.619452</td>\n",
       "      <td>3.734732</td>\n",
       "      <td>0.279622</td>\n",
       "      <td>41.876816</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.555174</td>\n",
       "      <td>3.709059</td>\n",
       "      <td>0.279731</td>\n",
       "      <td>40.815399</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.468799</td>\n",
       "      <td>3.706579</td>\n",
       "      <td>0.285298</td>\n",
       "      <td>40.714272</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.385066</td>\n",
       "      <td>3.703830</td>\n",
       "      <td>0.285549</td>\n",
       "      <td>40.602505</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.296635</td>\n",
       "      <td>3.736150</td>\n",
       "      <td>0.280691</td>\n",
       "      <td>41.936234</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.206934</td>\n",
       "      <td>3.744673</td>\n",
       "      <td>0.280599</td>\n",
       "      <td>42.295174</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.123483</td>\n",
       "      <td>3.761913</td>\n",
       "      <td>0.278713</td>\n",
       "      <td>43.030655</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.062740</td>\n",
       "      <td>3.762731</td>\n",
       "      <td>0.279681</td>\n",
       "      <td>43.065891</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.013705</td>\n",
       "      <td>3.763774</td>\n",
       "      <td>0.279372</td>\n",
       "      <td>43.110836</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_learn.unfreeze()\n",
    "llm_learn.fit_one_cycle(10, 1e-3) # 0.001 | https://iconof.com/1cycle-learning-rate-policy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e0f77fb-3f80-41b2-82db-de355a7c2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_learn.save_encoder('10epoch_finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d83fb99a-af3b-4774-b1ee-cc181a923c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The man is a good guy and has a lot of passion for merit , love and love . He has a great time and finds some good friends and family , but he has no great experience in it . He has a lot of have to do with some sort of\n"
     ]
    }
   ],
   "source": [
    "print(llm_learn.predict(\"The man is a good\", 50, temperature=0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0c76bca-cd6f-4324-8374-6d1278fb9b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "the_best_review_starts_with = \"I liked this movie because: \"\n",
    "n_words = 40\n",
    "n_sentences = 2\n",
    "preds = [llm_learn.predict(the_best_review_starts_with, n_words, temperature=0.75) \n",
    "         for _ in range(n_sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a03286e-5533-42cd-9ab1-2eb2837abb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i liked this movie because : \" it \\'s so hard to believe that this movie was made by a filmmaker . It was a matter of fact , but because of the length , it was not a movie or a movie .', \"i liked this movie because : i have a great idea of how this film could deal with family and family . It is a fascinating topic , because it has a lot of love and it 's not really quite an appropriate thing for\"]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0eb6ed-ac34-4d3e-bba1-ea1d7f9c9107",
   "metadata": {},
   "source": [
    "<img src=\"ulmfit.png\" alt=\"ULMFiT process\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bb066-bcf6-4cd0-ada9-5b7bad54cc3f",
   "metadata": {},
   "source": [
    "## Skip this part?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed9171a-21b8-494d-b039-73cbd7edde38",
   "metadata": {},
   "source": [
    "<img src=\"nevralt_nettverk.png\" alt=\"Nevralt nettverk\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d16e6ef-062c-47f6-8e30-bd7eca9f0947",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_classifier = TextDataLoaders.from_folder(subset_path, valid='test', text_vocab=dataloaders_lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b682caa0-3510-4ffc-93cc-a8307355a28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pythonki/lib/python3.12/site-packages/fastai/text/learner.py:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  wgts = torch.load(wgts_fname, map_location = lambda storage,loc: storage)\n"
     ]
    }
   ],
   "source": [
    "learn_2pass = text_classifier_learner(dataloaders_classifier, AWD_LSTM, drop_mult=0.5, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e792e2c2-301e-4f3a-8c49-4b20ae487e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pythonki/lib/python3.12/site-packages/fastai/text/learner.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  wgts = torch.load(join_path_file(file,self.path/self.model_dir, ext='.pth'), map_location=device)\n"
     ]
    }
   ],
   "source": [
    "encoder_path = subset_path/'unsup/models'\n",
    "learn_2pass = learn_2pass.load_encoder(encoder_path/'10epoch_finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10f290d1-a1d6-46ef-88c6-fa109c5cf4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.589310</td>\n",
       "      <td>0.628925</td>\n",
       "      <td>0.599609</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_2pass.fit_one_cycle(1, 2e-2) # 0.02 | https://iconof.com/1cycle-learning-rate-policy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65aadfb-1f99-45b7-9ac2-80cafa0c71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(slice(1e-2/(2.6**4),1e-2))\n",
    "#print(slice(5e-3/(2.6**4),5e-3))\n",
    "#print(slice(1e-3/(2.6**4),1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c968a28-1f26-46d8-a8cd-4475fa5425a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.464235</td>\n",
       "      <td>0.574443</td>\n",
       "      <td>0.697266</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_2pass.freeze_to(-2) # Last two layers\n",
    "learn_2pass.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2)) # epoch, lr group 0 (body), lr group 1 (head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d0efd54-7ef7-4aa1-85e6-fc297d7797fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.335465</td>\n",
       "      <td>0.503590</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_2pass.freeze_to(-3) # Last three layers\n",
    "learn_2pass.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3)) # epoch, lr group 0 (body), lr group 1 (head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b27c4709-906a-4794-aaa8-4b9eb81f8650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.237930</td>\n",
       "      <td>0.440052</td>\n",
       "      <td>0.791016</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.218334</td>\n",
       "      <td>0.418250</td>\n",
       "      <td>0.814453</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_2pass.unfreeze() # All layers\n",
    "learn_2pass.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3)) # epoch, lr group 0 (body), lr group 1 (head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d6720e4-4cd0-4303-958b-58d3e5084077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a: pos.\n",
      "Probability it's a positive: 0.9358\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a: neg.\n",
      "Probability it's a positive: 0.1404\n"
     ]
    }
   ],
   "source": [
    "# Use our model by passing it a review.\n",
    "category,_,probs = learn_2pass.predict(\"I really liked that movie\")\n",
    "\n",
    "print(f\"This is a: {category}.\")\n",
    "print(f\"Probability it's a positive: {probs[1]:.4f}\")\n",
    "\n",
    "category,_,probs = learn_2pass.predict(\"I did not like that movie, it was awful\")\n",
    "\n",
    "print(f\"This is a: {category}.\")\n",
    "print(f\"Probability it's a positive: {probs[1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
